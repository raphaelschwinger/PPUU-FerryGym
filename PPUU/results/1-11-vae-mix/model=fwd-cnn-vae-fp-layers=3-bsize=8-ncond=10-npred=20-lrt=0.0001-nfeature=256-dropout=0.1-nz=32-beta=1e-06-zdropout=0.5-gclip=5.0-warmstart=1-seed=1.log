2022-11-01 22:13:13.762723: step 2000 |  train loss [i: 0.00141, s: 0.00296, , p: 31.14098] valid loss [i: 0.00823, s: 0.01380, , p: 2.57508]
2022-11-01 23:26:49.902104: step 4000 |  train loss [i: nan, s: nan, , p: nan] valid loss [i: 0.00828, s: 0.00844, , p: 20.63738]
2022-11-02 00:44:19.058867: step 6000 |  train loss [i: 0.00156, s: 0.00275, , p: 50.34636] valid loss [i: 0.00776, s: 0.01012, , p: 4.60359]
2022-11-02 02:19:51.485414: step 8000 |  train loss [i: 0.00195, s: 0.00250, , p: 40.53785] valid loss [i: 0.00761, s: 0.00680, , p: 7.09296]
2022-11-02 04:03:55.546344: step 10000 |  train loss [i: 0.00143, s: 0.00241, , p: 19.98214] valid loss [i: 0.00758, s: 0.01155, , p: 5.14184]
2022-11-02 05:44:58.930996: step 12000 |  train loss [i: 0.00167, s: 0.00296, , p: 39.33496] valid loss [i: 0.00746, s: 0.00755, , p: 5.59558]
2022-11-02 07:24:55.911133: step 14000 |  train loss [i: 0.00288, s: 0.00447, , p: 88.67286] valid loss [i: 0.00791, s: 0.00442, , p: 86.72729]
2022-11-02 08:42:38.757631: step 16000 |  train loss [i: 0.00185, s: 0.00198, , p: 67.08614] valid loss [i: 0.00764, s: 0.00478, , p: 12.02572]
2022-11-02 09:55:00.453457: step 18000 |  train loss [i: 0.00141, s: 0.00266, , p: 26.80558] valid loss [i: 0.00824, s: 0.00522, , p: 7.49095]
2022-11-02 11:10:12.367007: step 20000 |  train loss [i: 0.00143, s: 0.00250, , p: 56.11909] valid loss [i: 0.00802, s: 0.00573, , p: 9.08422]
2022-11-02 12:23:34.013978: step 22000 |  train loss [i: nan, s: nan, , p: nan] valid loss [i: 0.00829, s: 0.00554, , p: 68.99546]
2022-11-02 13:35:31.778167: step 24000 |  train loss [i: 0.00166, s: 0.00223, , p: 43.38112] valid loss [i: 0.00817, s: 0.00534, , p: 11.47668]
2022-11-02 14:48:53.236763: step 26000 |  train loss [i: 0.00141, s: 0.00287, , p: 26.05389] valid loss [i: 0.00799, s: 0.00465, , p: 6.98155]
2022-11-02 16:12:41.370229: step 28000 |  train loss [i: 0.00149, s: 0.00233, , p: 26.39689] valid loss [i: 0.00755, s: 0.00436, , p: 34.03577]
